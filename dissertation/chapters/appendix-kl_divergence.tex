\section{Kullback-Leibler Divergence}%
\label{app:dkl:dkl}%

The entropy computes how uncertainty events are.
If a system is quite stable, the entropy will be close to zero and will not vary a lot, while if the system is quite unstable, then the entropy will be very large.
The Kullback-Leibler divergence, or relative entropy, is the measure of how different two separate probability distributions, $R$ and $P$, are from each other~\cite{bib:goodfellow2016}. Considering these probability distribution are both discrete and are defined on the same probability space $\chi$, the Kullback-Leibler divergence $D_{KL}(R || P)$ is given by
\begin{equation}
  \label{eq:app-dkl}
  D_{KL}(R || P) = \sum_{x \in \chi} R(x) \ln \left(\frac{R(x)}{P(x)} \right),
\end{equation}
the equation above can be read as the divergence from $P$ to $R$.

It is important to notice that this divergence measurement is not symmetric,
\begin{equation}
  \label{eq:app-dkl-nonsymmetric}
  \begin{split}
    D_{KL}(R||P) & \neq D_{KL}(P||R), \\
    \sum_{x \in \chi} R(x) \ln \left(\frac{R(x)}{P(x)} \right) & \neq \sum_{x \in \chi} P(x) \ln \left(\frac{P(x)}{R(x)} \right),
  \end{split}
\end{equation}
and that KL divergence, relative entropy, is always a non-negative value, 
\begin{equation}
  \label{eq:app-dkl-nonnegative}
  \begin{split}
    E & = D_{KL}(R||P) \\
    D_{KL}(R||P) & = \sum_{x \in \chi} R(x) \ln{\left( \frac{R(x)}{P(x)} \right)} \\
    & \geq \sum_{x \in \chi} R(x) \left( 1 - \frac{P(x)}{R(x)} \right) \\
    & = \sum_{x \in \chi} \left( R(x) - P(x) \right) \\
    & = \sum_{x \in \chi} R(x) - \sum_{x \in \chi} P(x) = 1 - 1 \\
    & \Rightarrow D_{KL}(R||P) \geq 0.
  \end{split}
\end{equation}

Notice that $D_{KL}(R||P)$ will only reach zero if and only if $P(x) = R(x)$, which means that we are able to retrieve the exactly desired probability distribution at the visible units from the input data.
