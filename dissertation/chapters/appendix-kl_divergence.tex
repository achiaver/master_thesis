\section{Kullback-Leibler Divergence}%
\label{app:dkl:dkl}%

The Kullback-Leibler divergence or relative entropy is the measure of the difference between two probability distributions. 
The entropy computes how uncertainty events are. 
If a system is quite stable, the entropy will be close to zero and will not vary a lot, while if the system is quite unstable, then the entropy will be very large. 


