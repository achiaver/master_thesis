

\section{New Ideas}
\subsection{ANSARI}

The energy computed in Boltzmann Machines is a measure of self-consistency of the system.
Randomness is fundamental to avoid local optima.
Exponential schedule for annealing instead of linear approach, does not have the drawback of premature freezing.
Premature freezing means that the system reaches minimum temperature and becomes deterministic before a global minimum has been reached.

Boltzmann machines can be seem as a system where each unit represents a hypothesis and each connection a constrain among hypothesis.

The ability of the system to escape local optimum depends on the path chosen for the temperature, the annealing schedule. The success of simulated annealing depends on the proper choice of the annealing schedule.
There is a need to have an annealing schedule where the temperature never gets stuck at a particular value, but varies dynamically in such a way so to increase the system optimality.


\subsection{SMOLENSKY}

\begin{equation}
    \label{eq:smolensky-weights}
    W_{i \alpha} = {\left( k_{\alpha} \right)}_{i} \frac{\sigma_{\alpha}}{|\mathbf{k}_{\alpha}|},
\end{equation}
where $W_{i \alpha}$ is the weight between atom $\alpha$ and feature $i$, $\mathbf{k}_{\alpha}$ is the connection vector of the atom $\alpha$, ${\left( k_{\alpha} \right)}_{i}$ is the connection vector to a unit $i$, and $\sigma_{\alpha}$ is the strenght of the atom $\alpha$.

The harmony function assigns a real number $H_{\mathbf{K}}(\mathbf{r}, \mathbf{a}) $ to each state of the system, which is determined by a pair $(\mathbf{r}, \mathbf{a})$, where $\mathbf{r}$ is the representation vector and $\mathbf{a}$ the activation vector.
\begin{equation}
    \label{eq:smolensky-harmonyfunction}
    H_{\mathbf{K}}(\mathbf{r}, \mathbf{a}) = \sum_{\alpha} \sigma_{\alpha} a_{\alpha} h_{\kappa}(\mathbf{r}, \mathbf{k}_{\alpha}),
\end{equation}
where $a_{\alpha}$ is the connection to atom $\alpha$, and $h_{\kappa}(\mathbf{r}, \mathbf{k}_{\alpha})$ is the harmony contribution by activating the atom $\alpha$ given the current representation vector $\mathbf{r}$.
\begin{equation}
    \label{eq:smolensky-harmonyatom}
    h_{\kappa}(\mathbf{r}, \mathbf{k}_{\alpha}) = \frac{\mathbf{r} \cdot \mathbf{k}_{\alpha}}{|\mathbf{k}_{\alpha}|} - \kappa
    = \frac{\langle \mathbf{r}, \mathbf{k}_{\alpha} \rangle}{|\mathbf{k}_{\alpha}|} - \kappa.
\end{equation}

Putting equation~(\ref{eq:smolensky-harmonyatom}) into equation~(\ref{eq:smolensky-harmonyfunction}),
\begin{equation}
  \label{eq:smolensky-harmonyfunction2}
  \begin{split}
      H_{\mathbf{K}}(\mathbf{r}, \mathbf{a}) & =  \sum_{\alpha} \sigma_{\alpha} a_{\alpha} \left[ \frac{\langle \mathbf{r}, \mathbf{k}_{\alpha} \rangle}{|\mathbf{k}_{\alpha}|} - \kappa \right] \\
      & = \sum_{\alpha} a_{\alpha} \left[ \frac{\sigma_{\alpha} \langle \mathbf{r}, \mathbf{k}_{\alpha} \rangle}{|\mathbf{k}_{\alpha}|} - \sigma_{\alpha} \kappa \right].
  \end{split}
 \end{equation}
Considering the inner product
\begin{equation}
    \label{eq:smolensky-innerproduct}
    \langle \mathbf{r}, \mathbf{k}_{\alpha} \rangle = \sum_{i} r_{i} {\left( k_{\alpha} \right)}_{i},
\end{equation}
and replacing equation~(\ref{eq:smolensky-innerproduct}) into equation~(\ref{eq:smolensky-harmonyfunction2}),
\begin{equation}
  \label{eq:smolensky-harmonyfunction3}
  \begin{split}
      H_{\mathbf{K}}(\mathbf{r}, \mathbf{a}) & = \sum_{\alpha} a_{\alpha} \left[ \frac{\sigma_{\alpha}}{|\mathbf{k}_{\alpha}|}  \sum_{i} r_{i} {\left( k_{\alpha} \right)}_{i} - \sigma_{\alpha} \kappa \right] \\
  & = \sum_{\alpha} a_{\alpha} \left[ \sum_{i} r_{i} {\left( k_{\alpha} \right)}_{i} \frac{\sigma_{\alpha}}{|\mathbf{k}_{\alpha}|} - \sigma_{\alpha} \kappa \right].
  \end{split}
\end{equation}

Equation~(\ref{eq:smolensky-harmonyfunction3}) we can be reorder thus it becomes a function of the weights $W_{i \alpha}$, equation~(\ref{eq:smolensky-weights}),
\begin{equation}
    \label{eq:smolensky-harmonyfunctionfinal}
    H_{\mathbf{K}}(\mathbf{r}, \mathbf{a}) = \sum_{\alpha} \sum_{i} r_{i} W_{i \alpha} a_{\alpha} - \sum_{\alpha} a_{\alpha} \sigma_{\alpha} \kappa,
\end{equation}
where it is possible to see the harmony function as a function of the representation vector $\mathbf{r}$, the activation vector $\mathbf{a}$ just as in equation~(\ref{eq:smolensky-harmonyfunction}), but also as a function of the weights $W_{i \alpha}$ and the bias $\gamma_{\alpha} = \sigma_{\alpha} \kappa$ which is connected to each atom.
Both weights and bias carry the information of the atom strength $\sigma_{\alpha}$, which is related to the frequency of the atom $\alpha$: the more a particular atom $\alpha$ is activated, the more its frequency, thus increasing the importance of this atom to describe a feature $i$ from the representation vector $\mathbf{r}$.


The maximum-likelihood completion function $c(\iota)$ determined by the probability distribution $p$ is defined by
\begin{equation}
    \label{eq:smolensky-maxcompletion}
    \begin{split}
        c(\iota) = \{\mathbf{r} \in \mathbf{R} \: | \: \text{for some} \: \mathbf{a} \in \mathbf{A}, \text{and all} \: (\mathbf{r}', \mathbf{a}') \in \mathbf{R} \times \mathbf{A} \\
        \text{such that} \: \mathbf{r}' \supset \iota: p(\mathbf{r}, \mathbf{a}) \geqslant p(\mathbf{r}', \mathbf{a}') \},
   \end{split}
\end{equation}
where $\mathbf{r}$ is the completion of a point $\iota$.

In information theory, the concept of entropy (missing information) can be interpreted as the average of uncertainty of a variable outcome, similarly in physics, entropy is relate to the randomness of a system or the lack of information about it.
Entropy is a function of the probability distribution of a random variable $x$:
\begin{equation}
    \label{eq:smolensky-entropy}
    S(P(x)) = - \sum_{x \in X} P(x) \ln{(P(x))}.
\end{equation}

Theorem $1$: \textit{competence}.

A cognitive system should performe completion according to Boltzmann distribution for statistical extrapolation and inference.
Considering an experiment, each pattern can be observed with a particular frequency.
Assuming the probability distribution of the experiment $R$ is consistent with the Boltzmann distribution $P$ with harmony function given by equation~(\ref{eq:smolensky-harmonyfunctionfinal}), the maximum-likelihood completions of this distribution are the same as
\begin{equation}
    \label{eq:smolensky-teo1}
    P(\mathbf{r}, \mathbf{a}) \propto e^{H(\mathbf{r}, \mathbf{a})}.
\end{equation}

Theorem $2$: \textit{realizability}.

Nodes are updates stochastically, every node within the same layer is updated in parallel, and then every node in the other layer is updated in parallel.
During the update process the computational temperature is lowered via simulated annealing.


\subsection{AGGARWAL}

RBM\: the probabilistic states of the network are learned for a set of inputs.
This network models the joint probability distribution os the observed attributes together with some hidden attributes.
The connections are undirected because, RBM are designed to learn te probabilistic relationship rather than input-output mapping.
RBM are probabilistic models that create latent representations of the underlying data points.
It is similar to an autoencoder, but the latent representation is created stochastically instead of deterministically.

Boltzmann machines use probabilistic states to represent Bernoulli distributions of the binary attributes.
It contains both visible and hidden states.

Hopfield: given the weights and biases in the network at any particular time, it is possible to find a local energy minimum in terms of the states by repeatedly using the update rule:
\begin{equation}
    \label{eq:aggarwal-hopfield-update}
    s_{i} =
    \begin{cases}
         1 \; \text{if} \; \sum_{j : j \ne i} \omega_{ij} s_{j} + b_{i} \geq 0 \\
         0 \; \text{otherwise}
     \end{cases}.
\end{equation}

For a particular set of parameters $(\omega_{ij}, b_{i})$, the Boltzmann machine defines a probability distribution over various state configurations.
In Boltzmann machines, the dependence between all pairs of states is undirected: the visible states depend as much on the hidden states as the hidden states depend on visible states.
Boltzmann machine iteratively samples the states using a conditional distribution generated from the state values in the previous iteration unil thermal equilibrium is reached.

Learnings weights in the Boltzmann machine requires to maximize the likelihood of the specific training dataset observed.
It is also possible to maximize the log-likelihood, by taking the $\ln$ of the probability distribution.
\begin{equation}
    \label{eq:aggarwal-log-likelihood}
    \ln(P(s)) = - E(s) - \ln(Z)
\end{equation}

To maximize the log-likelihood, equation~(\ref{eq:aggarwal-hopfield-update}) is derived in terms of the weights $\omega_{ij}$, then it follows
\begin{equation}
    \label{eq:aggarwal-log-likelihood-max}
    \frac{\partial \ln(P(s))}{\partial \omega_{ij}} = {\langle s_{i}, s_{j} \rangle}_{data} - {\langle s_{i}, s_{j}
    \rangle}_{model}.
\end{equation}

Weights' update rule rely on two terms: ${\langle s_{i}, s_{j} \rangle}_{data}$ which represents the correlations between the states of the nodes $i$ and $j$ when the visible vectors are fixed to a vector in the training data, and the hidden states are allowed to vary; the second term, ${\langle s_{i}, s_{j} \rangle}_{model}$ is an average product of $s_{i}$ and $s_{j}$ from the model-centric samples obtained from Gibbs sampling. 
\begin{equation}
    \label{eq:aggarwal-bm-weight-update}
    \omega_{ij} \leftarrow \omega_{ij} \alpha ({\langle s_{i}, s_{j} \rangle}_{data} - {\langle s_{i}, s_{j} \rangle}_{model}),
\end{equation}

The subtraction of the model-centric correlations (second term) is required in order to remove the effect of the partition function within the expression of the log probability.
Monte Carlo sampling requires a large amount of samples to reach thermal equilibrium.
Restricted Boltzmann machine is a simplification of the Boltzmann machine.

The probability of each hidden unit taking on the value $1$ is:
\begin{equation}
    \label{eq:aggarwal-rbm-hidden-prob}
    P(h_{j} | \mathbf{v}) = \frac{1}{1 + e^{-b^{(h)}_{j} - \sum_{i} v_{i} \omega_{ij}}},
\end{equation}
where energy gap $\Delta E_{j}$ of a hidden unit $j$, between values $h_{j} = 0$ and $h_{j} = 1$ is
\begin{equation}
    \label{eq:aggarwal-rbm-energy-gap-hidden}
    \Delta E_{j} = b_{j} + \sum_{i} v_{i} \omega_{ij},
\end{equation}
analogously we have the probability of each visible unit.

Equation~(\ref{eq:aggarwal-rbm-hidden-prob}) can also be writen in terms of the sigmoid
\begin{equation}
    \label{eq:aggarwal-rbm-hidden-prob-sigmoid}
    P(h_{j} | \mathbf{v}) = sigmoid(b_{j} + \sum_{i} v_{i} \omega_{ij}.
\end{equation}

For both the hidden and visible units, it is interesting to notice that the hidden unit $j$ depends only on the state of the visible units.
On the other hand, the visible unit $i$ depends only on the state of the hidden units.
This aspect makes it possible to update all hidden units in parallel and then all the visible units in parallel as well.

The weights encode the affinities between the visible and the hidden states.
Large positive weights implies the two states are likely to be on together.
Weights are initialized to a small value each.
Learning can be performed as follows:
\begin{itemize}
    \item \textit{Positive phase}: considering a mini-batch of training instances, for each element in this mini-batch, every hidden unit is updated according to equation~(\ref{eq:aggarwal-rbm-hidden-prob}).
    Once every hidden unit is updated, the average product between each pair of visible and hidden unit is computed, denoted as ${\langle v_{i}, h_{j} \rangle}_{pos}$.
    
    \item \textit{Negative phase}: for each training instance, it goes through a phase of Gibbs sampling after starting at with randomly initialized states.
    Is is achieved by repeatedly using equation~(\ref{eq:aggarwal-rbm-energy-gap-hidden}) and its version for the visible units.
    ${\langle v_{i}, h_{j} \rangle}_{neg}$ is computed after the values of $v_{i}$ and $h_{j}$ are at thermal equilibrium.
    
    \item Weights and biases can be updated as follows:
    \begin{equation}
        \label{eq:aggarwal-weights-bias-update}
        \begin{split}
            \omega_{ij} &\leftarrow \omega_{ij} + \alpha ({\langle v_{i}, h_{j} \rangle}_{pos} - {\langle v_{i}, h_{j} \rangle}_{neg}), \\
            b^{(v)}_{i} &\leftarrow b^{(v)}_{i} + \alpha ({\langle v_{i}, 1 \rangle}_{pos} - {\langle v_{i}, 1 \rangle}_{neg}), \\
            b^{(h)}_{j} &\leftarrow  b^{(h)}_{j} + \alpha ({\langle 1, h_{j} \rangle}_{pos} - {\langle 1, h_{j} \rangle}_{neg}), 
        \end{split}
    \end{equation}
    where $\alpha$ is the learning rate.
\end{itemize}

When visible unit $i$ and hidden unit $j$ are highly correlated, the weight $\omega_{ij}$ will be more positive, and the other way around, when visible unit $i$ and hidden unit $j$ are not correlated.


