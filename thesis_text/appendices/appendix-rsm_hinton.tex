\section{Replicated Softmax Model}%
\label{app:rsm}%

\citonline{bib:salakhutdinov-hinton2009} proposed the Replicated Softmax to model and extract low-dimensional latent semantic representations of an unstructured collection of documents.

Within Natural Language Processing research field it is possible to access the topic a document based on its words probability distribution, this is known as Topic Modelling.

Replicated Softmax model uses a Restricted Boltzmann Machine structure.

Considering a dictionary that has $K$ words, a document with $D$ words, and $\mathbf{h} \in {\{0, 1\}}^{F}$ stochastic hidden units.
Let $\mathbf{V}$ be an $D \times K$ observed binary matrix whose entries $v_{ik} = 1$ means the $i^{th}$ word of the document is the $k^{th}$ word of the dictionary, and $v_{ik} = 0$ otherwise.
For a state ${\{\mathbf{V}, \mathbf{h}\}}$, the energy is defined as follows:
\begin{equation}
    \label{eq:app:rsm-energy-Vh}
    E(\mathbf{V}, \mathbf{h}; \bm{\theta}) = - \sum^{D}_{i = 1} \sum^{F}_{j = 1} \sum^{K}_{k = 1} W_{ijk} h_{j} v_{ik} - \sum^{D}_{i = 1} \sum^{K}_{k = 1} v_{ik} b_{ik} - \sum^{F}_{j = 1} h_{j} a_{j},
\end{equation}
where $\bm{\theta} = \{\mathbf{W}, \mathbf{b}, \mathbf{a}\}$ is the set of model parameters.
$\mathbf{W}$ is the weight tensor, each element $W_{ijk}$ is a symmetric interaction between a visible unit $i$ that takes on value $k$ and a hidden unit $j$, $b_{ik}$ is the bias of unit $i$ that takes on value $k$, and $a_{j}$ is the bias of hidden unit $j$.

The normalized joint probability of state ${\{\mathbf{V}, \mathbf{h}\}}$ is given by:
\begin{equation}
    \label{eq:app:rsm-joint-prob}
    P(\mathbf{V}, \mathbf{h}; \bm{\theta}) = \frac{e^{-E(\mathbf{V}, \mathbf{h}; \bm{\theta})}}{Z(\bm{\theta})},
\end{equation}
where $Z$ is the partition function, the normalization constant to keep the probability within interval $[0, 1]$:
\begin{equation}
    \label{eq:app:rsm-partition-func}
    Z(\bm{\theta}) = \sum_{\mathbf{V}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{V}', \mathbf{h}'; \bm{\theta})}.
\end{equation}

The marginal probabilities the model given by $\bm{\theta} = \{\mathbf{W}, \mathbf{b}, \mathbf{a}\}$ assings to a visible binary matrix $\mathbf{V}$ and to a hidden binary vector $\mathbf{h}$ are, respectively:
\begin{align}
    \label{eq:app:rsm-marg-prob-visible}
    P(\mathbf{V}; \bm{\theta}) &= \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{V}, \mathbf{h}; \bm{\theta})}, \\
    \label{eq:app:rsm-marg-prob-hidden}
    P(\mathbf{h}; \bm{\theta}) &= \frac{1}{Z} \sum_{\mathbf{V}} e^{-E(\mathbf{V}, \mathbf{h}; \bm{\theta})}.
\end{align}

From the joint and marginal probability, we are able to derive the conditional probabilities for the hidden units $P(\mathbf{h} | \mathbf{V}; \bm{\theta})$ given the visible matrix $\mathbf{V}$:
\begin{equation}
    \label{eq:app:rsm-cond-prob-hidden}
    P(\mathbf{h} | \mathbf{V}; \bm{\theta}) = \frac{P(\mathbf{V}, \mathbf{h}; \bm{\theta})}{P(\mathbf{V}; \bm{\theta})}.
\end{equation}

For the sake of simplicity we will omit the term $\bm{\theta}$ by considering we are deriving the equations for a fixed model given by parameters $\bm{\theta}$, thus equation~(\ref{eq:app:rsm-cond-prob-hidden}) becomes: 
\begin{align}
    \label{eq:app:rsm-cond-prob-hidden-init}
    P(\mathbf{h} | \mathbf{V}) &= \frac{\frac{1}{Z} e^{-E(\mathbf{V}, \mathbf{h})}}{\frac{1}{Z} \sum\limits_{\mathbf{h}'} e^{-E(\mathbf{V}, \mathbf{h}')}} \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j} \right)}}{\sum\limits_{\mathbf{h}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h'_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h'_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j}\right) \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right)}}}{\left[ \sum\limits_{\mathbf{h}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h'_{j} v_{ik} + \sum\limits^{F}_{j=1} h'_{j} a_{j}\right)}\right] \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right)}} \nonumber \\ %
    &= \frac{\exp{\left[ \sum\limits^{F}_{j=1} h_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\sum\limits_{\mathbf{h}'} \exp{\left[ \sum\limits^{F}_{j=1} h'_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}} \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\sum\limits_{h'_{1} \in \{0,1\}} \cdots \sum\limits_{h'_{F} \in \{0,1\}} \left\{ \prod\limits^{F}_{j=1} \exp{\left[h'_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left\{ \sum\limits_{h'_{1} \atop \in \{0,1\}} \exp{\left[h'_{1} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{i1k} v_{ik} + a_{1}\right)\right]}\right\} \cdots \left\{ \sum\limits_{h'_{F} \atop \in \{0,1\}} \exp{\left[h'_{F} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{iFk} v_{ik} + a_{F}\right)\right]}\right\}}  \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{i1k} v_{ik} + a_{1}\right)}\right] \cdots \left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{iFk} v_{ik} + a_{F}\right)}\right]}  \nonumber \\ %
    &= \prod\limits^{F}_{j=1} \frac{\exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)}\right]}  \nonumber \\ %
    \label{eq:app:rsm-cond-prob-hidden-prod}
    P(\mathbf{h} | \mathbf{V}) &= \prod\limits^{F}_{j=1} P(h_{j} | \mathbf{V}),
\end{align}
where, considering the probability of $h_{j} = 1$ given $\mathbf{V}$, than equation~(\ref{eq:app:rsm-cond-prob-hidden-prod}) for a hidden unit $j$ is written as:
\begin{equation}
    \label{eq:app:rsm-hidden-logistic}
    P(h_{j} = 1 | \mathbf{V}) = \frac{\exp{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}}{\left[ 1 + \exp{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}\right]} = \sigma{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}, 
\end{equation}
which is also known as the logistic function $\sigma(x) = \frac{1}{1 + e^{-x}}$.
Thus, as in an common restricted Boltzmann Machine, each of the hidden units have an activation probability given by the logistic function.

On the other hand, the conditional probability of the visible units given the state of the hidden units $P(\mathbf{V} | \mathbf{h}; \bm{\theta})$ is given by:
\begin{equation}
    \label{eq:app:rsm-cond-prob-visible}
    P(\mathbf{V} | \mathbf{h}; \bm{\theta}) = \frac{P(\mathbf{V}, \mathbf{h}; \bm{\theta})}{P(\mathbf{h}; \bm{\theta})}.
\end{equation}

Again we consider a fixed model $\bm{\theta}$, thus this term is omitted from now on. 
The following derivation of equation~(\ref{eq:app:rsm-cond-prob-visible}) is not quite straight forward, and we will try to keep it as clear as possible:
\begin{align}
    \label{eq:app:rsm-cond-prob-visible-init}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\frac{1}{Z} e^{-E(\mathbf{V}, \mathbf{h})}}{\frac{1}{Z} \sum\limits_{\mathbf{V}'} e^{-E(\mathbf{V}', \mathbf{h})}} \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j} \right)}}{\sum\limits_{\mathbf{V}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v'_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right) \exp{\left( \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}}}{\left[ \sum\limits_{\mathbf{V}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v'_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} b_{ik}\right)}\right] \exp{\left( \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left[ \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'} \exp{\left[ \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'} \left\{ \prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'_{1}} \sum\limits_{\mathbf{V}'_{2}} \cdots \sum\limits_{\mathbf{V}'_{D}} \left\{ \prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\left\{\sum\limits_{\mathbf{V}'_{1}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{1k} \left( \sum\limits^{F}_{j=1} W_{1jk} h_{j} + b_{1k}\right)\right]}\right\} \cdots \left\{\sum\limits_{\mathbf{V}'_{D}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{Dk} \left( \sum\limits^{F}_{j=1} W_{Djk} h_{j} + b_{Dk}\right)\right]}\right\}} \nonumber \\ %
    \label{eq:app:rsm-cond-prob-visible-mid}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left\{\sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}}
\end{align}

To contunue with derivation of equation~(\ref{eq:app:rsm-cond-prob-visible-mid}), we will take a closer look at the denominator.
For a document $\mathbf{V}$, in which there are $D$ words positions that are chosen from a dictionary of $K$ available words, when $v_{ik} = 1$, it means that the $i^{th}$ word of the document is the $k^{th}$ word of the dictionary, and $0$ otherwise.
Whenever we sum over all $K$ words of the dictionary, this sum is $1$ for each word $i$ of the document, $\sum^{K}_{k=1} v_{ik} = 1$.
In this case we exclude empty documents, and even for a single word document, the summation above will be $1$.
In order to help with readability, we consider the simplification presented on (JORG THESIS, 2011), we take $r_{ik} = \sum^{F}_{j=1} W_{ijk} h_{j} + b_{ik}$, then, for a particular word $i$ of the document, the denominator of equation~(\ref{eq:app:rsm-cond-prob-visible-mid}) becomes:
\begin{align}
    \sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]} &= \sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left( v'_{ik} r_{ik} \right)} \nonumber \\ %
    &= \sum\limits_{\mathbf{V}'_{i}} \left[ \exp{\left(v_{i1} r_{i1}\right)} \exp{\left(v_{i2} r_{i2}\right)} \dots \exp{\left(v_{iK} r_{iK}\right)} \right] \nonumber \\ %
    = \left[ \exp{\left(r_{i1} (1)\right)} \exp{\left(r_{i2} (0)\right)} \dots \exp{\left(r_{iK} (0)\right)} \right] &+ \left[ \exp{\left(r_{i1} (0)\right)} \exp{\left(r_{i2} (1)\right)} \dots \exp{\left(r_{iK} (0)\right)} \right] + \nonumber \\ %
    \dots &+ \left[ \exp{\left(r_{i1} (0)\right)} \exp{\left(r_{i2} (0)\right)} \dots \exp{\left(r_{iK} (1)\right)} \right] \nonumber \\ %
    &= \sum\limits^{K}_{q=1} \exp{\left(r_{iq}\right)} \nonumber \\ %
    \label{eq:app:rsm-cond-prob-vis-deno}
    &= \sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}.
\end{align}

Equation~(\ref{eq:app:rsm-cond-prob-vis-deno}) can be replaced at equation~(\ref{eq:app:rsm-cond-prob-visible-mid}) denominator:
\begin{align}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left\{\sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left[ \sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)} \right]} \nonumber \\ %
    P(\mathbf{V} | \mathbf{h}) &= \prod\limits^{D}_{i=1} \frac{\prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}}.
\end{align}

Considering we would like to know the probability of having visible unit $i$ on at word $k$, $v_{ik} = 1$, given the hidden units state $\mathbf{h}$:
\begin{equation}
    \label{eq:app:rsm-visible-softmax}
    P(v_{ik} = 1 | \mathbf{h}) = \frac{\exp{\left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)}}{\sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}},
\end{equation}
which is the softmax function.

\input{images/tikz_figs/rsm_fig2.tex}

\input{images/tikz_figs/rsm_fig3.tex}
