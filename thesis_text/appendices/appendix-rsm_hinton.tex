\section{Replicated Softmax Model}%
\label{app:rsm}%
%
Salakhutdinov and Hinton~(\citeyear{bib:salakhutdinov-hinton2009}) proposed the Replicated Softmax to model and extract low-dimensional latent semantic representations of an unstructured collection of documents.
The authors applied the Replicated Softmax to a Natural Language Problem known as topic modelling where the topic of a document is defined based on its words probability distribution.
As a basic engine, the Replicated Softmax uses a variation of the Restricted Boltzmann Machine.

Salakhutdinov~(\citeyear{bib:salakhutdinov2015}) published a compilation of Generative Models, and we will consider his notation in the followin explanation.

Considering a dictionary that has $K$ words, a document with $D$ words, and $\mathbf{h} \in {\{0, 1\}}^{F}$ stochastic hidden units.
Let $\mathbf{V}$ be an $D \times K$ observed binary matrix whose entries $v_{ik} = 1$ means the $i^{th}$ word of the document is the $k^{th}$ word of the dictionary, and $v_{ik} = 0$ otherwise.
For a state ${\{\mathbf{V}, \mathbf{h}\}}$, the energy is defined as follows:
\begin{equation}
    \label{eq:app:rsm-energy-Vh}
    E(\mathbf{V}, \mathbf{h}) = - \sum^{D}_{i = 1} \sum^{F}_{j = 1} \sum^{K}_{k = 1} W_{ijk} h_{j} v_{ik} - \sum^{D}_{i = 1} \sum^{K}_{k = 1} v_{ik} b_{ik} - D\sum^{F}_{j = 1} h_{j} a_{j},
\end{equation}
where the set of parameters $\{\mathbf{W}, \mathbf{b}, \mathbf{a}\}$ refers to the set of model parameters, or network parameters.
$\mathbf{W}$ is the weight tensor, each element $W_{ijk}$ is a symmetric interaction between a visible unit $i$ that takes on value $k$ and a hidden unit $j$, $b_{ik}$ is the bias of unit $i$ that takes on value $k$, and $a_{j}$ is the bias of hidden unit $j$.

The normalized joint probability of state ${\{\mathbf{V}, \mathbf{h}\}}$ is given by:
\begin{equation}
    \label{eq:app:rsm-joint-prob}
    P(\mathbf{V}, \mathbf{h}) = \frac{e^{-E(\mathbf{V}, \mathbf{h})}}{Z},
\end{equation}
where $Z$ is the partition function, the normalization constant to keep the probability within interval $[0, 1]$:
\begin{equation}
    \label{eq:app:rsm-partition-func}
    Z = \sum_{\mathbf{V}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{V}', \mathbf{h}')}.
\end{equation}

The marginal probabilities of the visible binary matrix $\mathbf{V}$ and of the hidden binary vector $\mathbf{h}$ are, respectively:
\begin{align}
    \label{eq:app:rsm-marg-prob-visible}
    P(\mathbf{V}) &= \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{V}, \mathbf{h})}, \\
    \label{eq:app:rsm-marg-prob-hidden}
    P(\mathbf{h}) &= \frac{1}{Z} \sum_{\mathbf{V}} e^{-E(\mathbf{V}, \mathbf{h})}.
\end{align}

From the joint and marginal probability, we are able to derive the conditional probabilities for the hidden units $P(\mathbf{h} | \mathbf{V})$ given the visible matrix $\mathbf{V}$:
\begin{equation}
    \label{eq:app:rsm-cond-prob-hidden}
    P(\mathbf{h} | \mathbf{V}) = \frac{P(\mathbf{V}, \mathbf{h})}{P(\mathbf{V})}.
\end{equation}

Equation~(\ref{eq:app:rsm-cond-prob-hidden}), by definition, devides equation~(\ref{eq:app:rsm-joint-prob}) by equation~(\ref{eq:app:rsm-marg-prob-visible}):
\begin{align}
    \label{eq:app:rsm-cond-prob-hidden-init}
    P(\mathbf{h} | \mathbf{V}) &= \frac{\frac{1}{Z} e^{-E(\mathbf{V}, \mathbf{h})}}{\frac{1}{Z} \sum\limits_{\mathbf{h}'} e^{-E(\mathbf{V}, \mathbf{h}')}} \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j} \right)}}{\sum\limits_{\mathbf{h}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h'_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h'_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j}\right) \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right)}}}{\left[ \sum\limits_{\mathbf{h}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h'_{j} v_{ik} + \sum\limits^{F}_{j=1} h'_{j} a_{j}\right)}\right] \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right)}} \nonumber \\ %
    &= \frac{\exp{\left[ \sum\limits^{F}_{j=1} h_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\sum\limits_{\mathbf{h}'} \exp{\left[ \sum\limits^{F}_{j=1} h'_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}} \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\sum\limits_{h'_{1} \in \{0,1\}} \cdots \sum\limits_{h'_{F} \in \{0,1\}} \left\{ \prod\limits^{F}_{j=1} \exp{\left[h'_{j} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left\{ \sum\limits_{\substack{h'_{1} \\ \in \{0,1\}}} \exp{\left[h'_{1} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{i1k} v_{ik} + a_{1}\right)\right]}\right\} \cdots \left\{ \sum\limits_{\substack{h'_{F} \\ \in \{0,1\}}} \exp{\left[h'_{F} \left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{iFk} v_{ik} + a_{F}\right)\right]}\right\}}  \nonumber \\ %
    &= \frac{\prod\limits^{F}_{j=1} \exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{i1k} v_{ik} + a_{1}\right)}\right] \cdots \left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{iFk} v_{ik} + a_{F}\right)}\right]}  \nonumber \\ %
    &= \prod\limits^{F}_{j=1} \frac{\exp{\left[h_{j} \left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)\right]}}{\left[ 1 + \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j}\right)}\right]}  \nonumber \\ %
    \label{eq:app:rsm-cond-prob-hidden-prod}
    P(\mathbf{h} | \mathbf{V}) &= \prod\limits^{F}_{j=1} P(h_{j} | \mathbf{V}),
\end{align}
where, considering the probability of $h_{j} = 1$ given $\mathbf{V}$, than equation~(\ref{eq:app:rsm-cond-prob-hidden-prod}) for a hidden unit $j$ is written as:
\begin{equation}
    \label{eq:app:rsm-hidden-logistic}
    P(h_{j} = 1 | \mathbf{V}) = \frac{\exp{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}}{\left[ 1 + \exp{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}\right]} = \sigma{\left(\sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} W_{ijk} v_{ik} + a_{j} \right)}, 
\end{equation}
which is also known as the logistic function $\sigma(x) = \frac{1}{1 + e^{-x}}$.
Thus, as in an common restricted Boltzmann Machine, each of the hidden units have an activation probability given by the logistic function.

On the other hand, the conditional probability of the visible units given the state of the hidden units $P(\mathbf{V} | \mathbf{h})$ is given by:
\begin{equation}
    \label{eq:app:rsm-cond-prob-visible}
    P(\mathbf{V} | \mathbf{h}) = \frac{P(\mathbf{V}, \mathbf{h})}{P(\mathbf{h})}.
\end{equation}

The following derivation of equation~(\ref{eq:app:rsm-cond-prob-visible}) is not quite straight forward, and we will try to keep it as clear as possible:
\begin{align}
    \label{eq:app:rsm-cond-prob-visible-init}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\frac{1}{Z} e^{-E(\mathbf{V}, \mathbf{h})}}{\frac{1}{Z} \sum\limits_{\mathbf{V}'} e^{-E(\mathbf{V}', \mathbf{h})}} \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j} \right)}}{\sum\limits_{\mathbf{V}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v'_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} b_{ik} + \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} b_{ik}\right) \exp{\left( \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}}}{\left[ \sum\limits_{\mathbf{V}'} \exp{\left( \sum\limits^{D}_{i=1} \sum\limits^{F}_{j=1} \sum\limits^{K}_{k=1} W_{ijk} h_{j} v'_{ik} + \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} b_{ik}\right)}\right] \exp{\left( \sum\limits^{F}_{j=1} h_{j} a_{j}\right)}} \nonumber \\ %
    &= \frac{\exp{\left[ \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'} \exp{\left[ \sum\limits^{D}_{i=1} \sum\limits^{K}_{k=1} v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'} \left\{ \prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits_{\mathbf{V}'_{1}} \sum\limits_{\mathbf{V}'_{2}} \cdots \sum\limits_{\mathbf{V}'_{D}} \left\{ \prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\left\{\sum\limits_{\mathbf{V}'_{1}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{1k} \left( \sum\limits^{F}_{j=1} W_{1jk} h_{j} + b_{1k}\right)\right]}\right\} \cdots \left\{\sum\limits_{\mathbf{V}'_{D}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{Dk} \left( \sum\limits^{F}_{j=1} W_{Djk} h_{j} + b_{Dk}\right)\right]}\right\}} \nonumber \\ %
    \label{eq:app:rsm-cond-prob-visible-mid}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left\{\sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}}
\end{align}

To contunue with derivation of equation~(\ref{eq:app:rsm-cond-prob-visible-mid}), we will take a closer look at the denominator.
For a document $\mathbf{V}$, in which there are $D$ words positions that are chosen from a dictionary of $K$ available words, when $v_{ik} = 1$, it means that the $i^{th}$ word of the document is the $k^{th}$ word of the dictionary, and $0$ otherwise.
Whenever we sum over all $K$ words of the dictionary, this sum is $1$ for each word $i$ of the document, $\sum^{K}_{k=1} v_{ik} = 1$.
In this case we exclude empty documents, and even for a single word document, the summation above will be $1$.
In order to help with readability, we consider the simplification presented on (JORG THESIS, 2011), we take $r_{ik} = \sum^{F}_{j=1} W_{ijk} h_{j} + b_{ik}$, then, for a particular word $i$ of the document, the denominator of equation~(\ref{eq:app:rsm-cond-prob-visible-mid}) becomes:
\begin{align}
    \sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]} &= \sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left( v'_{ik} r_{ik} \right)} \nonumber \\ %
    &= \sum\limits_{\mathbf{V}'_{i}} \left[ \exp{\left(v_{i1} r_{i1}\right)} \exp{\left(v_{i2} r_{i2}\right)} \dots \exp{\left(v_{iK} r_{iK}\right)} \right] \nonumber \\ %
    = \left[ \exp{\left(r_{i1} (1)\right)} \exp{\left(r_{i2} (0)\right)} \dots \exp{\left(r_{iK} (0)\right)} \right] &+ \left[ \exp{\left(r_{i1} (0)\right)} \exp{\left(r_{i2} (1)\right)} \dots \exp{\left(r_{iK} (0)\right)} \right] + \nonumber \\ %
    \dots &+ \left[ \exp{\left(r_{i1} (0)\right)} \exp{\left(r_{i2} (0)\right)} \dots \exp{\left(r_{iK} (1)\right)} \right] \nonumber \\ %
    &= \sum\limits^{K}_{q=1} \exp{\left(r_{iq}\right)} \nonumber \\ %
    \label{eq:app:rsm-cond-prob-vis-deno}
    &= \sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}.
\end{align}

Equation~(\ref{eq:app:rsm-cond-prob-vis-deno}) can be replaced at equation~(\ref{eq:app:rsm-cond-prob-visible-mid}) denominator:
\begin{align}
    P(\mathbf{V} | \mathbf{h}) &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left\{\sum\limits_{\mathbf{V}'_{i}} \prod\limits^{K}_{k=1} \exp{\left[ v'_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}\right\}} \nonumber \\ %
    &= \frac{\prod\limits^{D}_{i=1} \prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\prod\limits^{D}_{i=1} \left[ \sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)} \right]} \nonumber \\ %
    P(\mathbf{V} | \mathbf{h}) &= \prod\limits^{D}_{i=1} \frac{\prod\limits^{K}_{k=1} \exp{\left[ v_{ik} \left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)\right]}}{\sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}}.
\end{align}

Considering we would like to know the probability of having visible unit $i$ on at word $k$, $v_{ik} = 1$, given the hidden units state $\mathbf{h}$:
\begin{equation}
    \label{eq:app:rsm-visible-softmax}
    P(v_{ik} = 1 | \mathbf{h}) = \frac{\exp{\left( \sum\limits^{F}_{j=1} W_{ijk} h_{j} + b_{ik}\right)}}{\sum\limits^{K}_{q=1} \exp{\left( \sum\limits^{F}_{j=1} W_{ijq} h_{j} + b_{iq}\right)}},
\end{equation}
which is the softmax function.

Figure~(\ref{fig:rsm-fig2}) shows the Replicated Softmax model describe up to this point. The model has its visible and hidden units, respectively $v_{ik}$ and $h_{j}$, which are connected by weights $w_{ijk}$. 

In a set of $N$ different documents, $\{\mathbf{V}_{n}\}^{N}_{n=1}$, each document $V_{n}$ has its own RBM. Considering the word order in the document is irrelevant, for the $k^{th}$ word of the dictionary, every $i^{th}$ word of the document which is activated on $k$ share the same weights. The assumption of word order irrelevance, implies that $W_{ijk}$ = $W_{jk}$ and $b_{ik} = b_{k}$, i.e., the weights that connect every unit of $k$ to a unit $j$ are the same, and so is the bias. For a document with $D$ words, the energy of the state $\{\mathbf{V}, \mathbf{h}\}$ is equivalent of the state $\{\mathbf{\hat{V}}, \mathbf{h}\}$:
\begin{align}
    \label{eq:app:rsm-energy-shareweights}
     E(\mathbf{V}, \mathbf{h}) &= -\sum^{D}_{i=1} \sum^{F}_{j=1} \sum^{K}_{k=1} W_{ijk} h_{j} v_{ik} - \sum^{D}_{i=1} \sum^{K}_{k=1} v_{ik} b_{k} - D \sum^{F}_{j=1} h_{j} a_{j} \nonumber \\ %
     &= -\sum^{F}_{j=1} \sum^{K}_{k=1} W_{jk} h_{j} \left[ \sum^{D}_{i=1} v_{ik} \right] - \sum^{K}_{k=1}  \left[ \sum^{D}_{i=1 }v_{ik} \right] b_{k} - D \sum^{F}_{j=1} h_{j} a_{j} \nonumber \\ %
     E(\mathbf{\hat{V}}, \mathbf{h}) &= -\sum^{F}_{j=1} \sum^{K}_{k=1} W_{jk} h_{j} \hat{v}_{k} - \sum^{K}_{k=1} \hat{v}_{k} b_{k} - D \sum^{F}_{j=1} h_{j} a_{j}, %
\end{align}
where $\hat{v}_{k} = \sum^{D}_{i=1} v_{ik}$ is the count or frequency of the $k^{th}$ word of the dictionary. It is important to notice that in both equations~(\ref{eq:app:rsm-energy-Vh}) and (\ref{eq:app:rsm-energy-shareweights}) the hidden units term (third term on the right side of the equations) is scaled by the length $D$ of the document, each RBM will have its own $D$ scaler. By applying this scaler, the energy becomes sensitive to the size of the document, and thus it is able to handle different size documents.

In the collection of documents $\{\mathbf{V}_{n}\}^{N}_{n=1}$, each document $\mathbf{V}_{n}$ has a desired probability $R(\mathbf{V}_{n})$, while the network will is able to provide the marginal probability $P(\mathbf{V}_{n})$. The relative entropy or Kullback-Leibler divergence can be used to estimate the difference between the distributions $P(\mathbf{\hat{V}})$ and $R(\mathbf{\hat{V}})$ for the $n^{th}$ document with $D$ words,
\begin{equation}
    \label{eq:app:rsm-dkl}
    D_{KL}(R(\mathbf{\hat{V}} || P(\mathbf{\hat{V}}) = \sum_{\mathbf{\hat{V}}} R(\mathbf{\hat{V}}) \ln\left[ \frac{R(\mathbf{\hat{V}})}{P(\mathbf{\hat{V}})} \right].
\end{equation}

The divergence at equation~(\ref{eq:app:rsm-dkl}) can be minimize by finding the optimal set of parameters $\{\mathbf{W}, \mathbf{b}, \mathbf{a}\}$. Theses parameters can be obtain by using the gradient descent:
\begin{align}
    \label{eq:app:rsm-wjk}
    \Delta W_{jk} &= -\eta \frac{\partial D_{KL}}{\partial W_{jk}}, \\ %
    \label{eq:app:rsm-bk}
    \Delta b_{k} &= -\eta_{b} \frac{\partial D_{KL}}{\partial b_{k}}, \\ %
    \label{eq:app:rsm-aj}
    \Delta a_{j} &= -\eta_{a} \frac{\partial D_{KL}}{\partial a_{j}}.
\end{align}

The derivation of equation~(\ref{eq:app:rsm-wjk}) follows as
\begin{align}
    \Delta W_{jk} &= -\eta \frac{\partial }{\partial W_{jk}} \left[ D_{KL}(R(\mathbf{\hat{V}}) || P(\mathbf{\hat{V}})) \right] \nonumber \\ %
    &= -\eta \frac{\partial}{\partial W_{jk}} \left[ \sum_{\mathbf{\hat{V}}} R(\mathbf{\hat{V}}) \left[ \ln\left(R(\mathbf{\hat{V}})\right) - \ln\left(P(\mathbf{\hat{V}})\right) \right] \right] \nonumber \\ %
    &= \eta \frac{\partial}{\partial W_{jk}} \left[ \sum_{\mathbf{\hat{V}}} R(\mathbf{\hat{V}}) \ln\left(P(\mathbf{\hat{V}})\right) \right] \nonumber \\ %
    \label{eq:app:rsm-wjk-step1}
    &= \eta \sum_{\mathbf{\hat{V}}} R(\mathbf{\hat{V}}) \frac{\partial}{\partial W_{jk}} \left[ \ln\left(P(\mathbf{\hat{V}})\right) \right]. %
\end{align}

At equation~(\ref{eq:app:rsm-wjk-step1}), working only with the derivative,
\begin{align}
    \frac{\partial}{\partial W_{jk}} \left[ \ln\left(P(\mathbf{\hat{V}})\right) \right] &= \frac{\partial}{\partial W_{jk}} \left[ \sum_{\mathbf{h}} \frac{e^{-E(\mathbf{\hat{V}}, \mathbf{h})}}{Z} \right] \nonumber \\ %
    \label{eq:app:rsm-wjk-step2}
    &= \frac{1}{Z} \sum_{\mathbf{h}} \frac{\partial}{\partial W_{jk}} \left[ e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \right] - \frac{\sum_{\mathbf{h}} e^{-E(\mathbf{\hat{V}}, \mathbf{h})}}{Z^{2}} \frac{\partial}{\partial W_{jk}} \left[ \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \right],
\end{align}
then at equation~(\ref{eq:app:rsm-wjk-step2}), for each of the chain derivatives we have
\begin{align}
    \frac{\partial}{\partial W_{jk}} \left[ e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \right] &= e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \frac{\partial}{\partial W_{jk}} \left[ -E(\mathbf{\hat{V}}, \mathbf{h}) \right] \nonumber \\ %
    &= e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \frac{\partial}{\partial W_{jk}} \left[ \sum^{F}_{l=1} \sum^{K}_{m=1} W_{lm} h_{l} \hat{v}_{m} + \sum^{K}_{m=1} \hat{v}_{m} b_{m} + D \sum^{F}_{l=1} h_{l} a_{l} \right] \nonumber \\ %
    &= e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \frac{\partial}{\partial W_{jk}} \left[ \sum_{l\neq j} \sum_{m \neq k} W_{lm} h_{l} \hat{v}_{m} + \sum^{K}_{m=1} W_{jm} h_{j} \hat{v}_{m} + \sum^{F}_{l=1} W_{lk} h_{l} \hat{v}_{k} + W_{jk} h_{j} \hat{v_{k}} \right] \nonumber \\ %
    \label{eq:app:rsm-wjk-step3}
    &= e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \hat{v}_{k} h_{j}, 
\end{align}
and for the second term with a chain derivative,
\begin{align}
    \frac{\partial}{\partial W_{jk}} \left[ \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \right] &= \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \frac{\partial}{\partial W_{jk}} \left[ -E(\mathbf{\hat{V}}', \mathbf{h}') \right] \nonumber \\ %
    &= \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \frac{\partial}{\partial W_{jk}} \left[ \sum^{F}_{l=1} \sum^{K}_{m=1} W_{lm} h'_{l} \hat{v}'_{m} + \sum^{K}_{m=1} \hat{v}'_{m} b_{m} + D \sum^{F}_{l=1} h'_{l} a_{l} \right] \nonumber \\ %
    \label{eq:app:rsm-wjk-step4}
    &= \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \hat{v}'_{k} h'_{j}. 
\end{align}

In the next steps, we just bring back the results of equations~(\ref{eq:app:rsm-wjk-step3}) and (\ref{eq:app:rsm-wjk-step4}) to equation~(\ref{eq:app:rsm-wjk-step2}), and then to equation~(\ref{eq:app:rsm-wjk-step1}),
\begin{align}
    \frac{\partial}{\partial W_{jk}} \left[ \ln\left(P(\mathbf{\hat{V}})\right) \right] &= \frac{1}{Z} \sum_{\mathbf{h}} e^{-E(\mathbf{\hat{V}}, \mathbf{h})} \hat{v}_{k} h_{j} - \frac{\sum_{\mathbf{h}} e^{-E(\mathbf{\hat{v}}, \mathbf{h})}}{Z^{2}} \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} e^{-E(\mathbf{\hat{V}}', \mathbf{h}')} \hat{v}'_{k} h'_{j} \nonumber \\ % 
    &= \sum_{\mathbf{h}} \frac{e^{-E(\mathbf{\hat{V}}, \mathbf{h})}}{Z} \hat{v}_{k} h_{j} - \sum_{\mathbf{h}} \frac{e^{-E(\mathbf{\hat{V}}, \mathbf{h})}}{Z} \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} \frac{e^{-E(\mathbf{\hat{V}}', \mathbf{h}')}}{Z} \hat{v}'_{k} h'_{j} \nonumber \\ %
    \label{eq:app:rsm-wjk-step5}
    &= \sum_{\mathbf{h}} P(\mathbf{\hat{V}}, \mathbf{h}) \hat{v}_{k} h_{j} - P(\mathbf{\hat{V}}) \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} P(\mathbf{\hat{V}}', \mathbf{h}') \hat{v}'_{k} h'_{j},
\end{align}
then the increment $\Delta W_{jk}$ is
\begin{align}
    \Delta W_{jk} &= \eta \sum_{\mathbf{\hat{V}}} \frac{R(\mathbf{\hat{V}})}{P(\mathbf{\hat{V}})} \left[ \sum_{\mathbf{h}} P(\mathbf{\hat{V}}, \mathbf{h}) \hat{v}_{k} h_{j} - P(\mathbf{\hat{V}}) \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} P(\mathbf{\hat{V}}', \mathbf{h}') \hat{v}'_{k} h'_{j} \right] \nonumber \\ %
    &= \eta \left[ \sum_{\mathbf{\hat{V}}} \sum_{\mathbf{h}} \frac{R(\mathbf{\hat{V}})}{P(\mathbf{\hat{V}})} P(\mathbf{\hat{V}}, \mathbf{h}) \hat{v}_{k} h_{j} - \sum_{\mathbf{\hat{V}}} \frac{R(\mathbf{\hat{V}})}{P(\mathbf{\hat{V}})} P(\mathbf{\hat{V}}) \sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} P(\mathbf{\hat{V}}', \mathbf{h}') \hat{v}'_{k} h'_{j} \right] \nonumber \\ %
    \label{eq:app:rsm-wjk-step6}
    &= \eta \left[ \sum_{\mathbf{\hat{V}}} \sum_{\mathbf{h}} R(\mathbf{\hat{V}}) P(\mathbf{h}|\mathbf{\hat{V}}) \hat{v}_{k} h_{j} - \overbrace{\sum_{\mathbf{\hat{V}}} R(\mathbf{\hat{V}})}^{=1} \underbrace{\sum_{\mathbf{\hat{V}}'} \sum_{\mathbf{h}'} P(\mathbf{\hat{V}}', \mathbf{h}') \hat{v}'_{k} h'_{j}}_{\langle \hat{v}'_{k} h'_{j} \rangle_{model}} \right], 
\end{align}
where $\langle \hat{v}_{k} h_{j} \rangle_{model}$ is the weighted average or expected value of the network.

When modeling the desired probability $R(\mathbf{\hat{V}})$, we expect that the network is able to represent the desired conditional probability of the hidden units $\mathbf{h}$ given a certain state of the visible units $\mathbf{\hat{V}}$, which means that we would like to have
\begin{align}
    \label{eq:app:rsm-desired-conditional-hidden}
    P(\mathbf{h}|\mathbf{\hat{V}}) &= R(\mathbf{h}|\mathbf{\hat{V}}), \\ %
    P(\mathbf{h}|\mathbf{\hat{V}}) &= \frac{R(\mathbf{\hat{V}}, \mathbf{h})}{R(\mathbf{\hat{V}})}, \nonumber \\ %
    \label{eq:app:rsm-wjk-likelihood}
    \implies R(\mathbf{\hat{V}}) P(\mathbf{h}|\mathbf{\hat{V}}) &= R(\mathbf{\hat{V}}, \mathbf{h}). 
\end{align}

With this last step, we can retrieve the equation of the increment of the weights $W_{jk}$,
\begin{align}
    \Delta W_{jk} &= \eta \left[ \overbrace{\sum_{\mathbf{\hat{V}}} \sum_{\mathbf{h}} R(\mathbf{\hat{V}}, \mathbf{\hat{V}}) \hat{v}_{k} h_{j}}^{\langle \hat{v}_{k} h_{j} \rangle_{data}} - \langle \hat{v}'_{k} h'_{j} \rangle_{model} \right], \nonumber \\ %
    \therefore \Delta W_{jk} &= \eta \left[ \langle \hat{v}_{k} h_{j} \rangle_{data} - \langle \hat{v}'_{k} h'_{j} \rangle_{model} \right].
\end{align}




\input{images/tikz_figs/rsm_fig2.tex}
\input{images/tikz_figs/rsm_fig3.tex} 

